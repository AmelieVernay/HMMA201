---
title: 'Régression Multiple - Sélection de Variable'
author: "Paul Bastide - Benjamin Charlier"
date: "15/02/2021"
output:
  ioslides_presentation:
    fig_width: 7
    fig_height: 4
  self_contained: true
---
<!-- ************************************************************************ -->
# What we know
<!-- ************************************************************************ -->

## Gaussian Model

### Model:
$$
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
$$

* $\mathbf{y}$ **random** vector of $n$ responses
* $\mathbf{X}$ **non random** $n\times p$ matrix of predictors
* $\boldsymbol{\epsilon}$ **random** vector of $n$ errors
* $\boldsymbol{\beta}$ **non random, unknown** vector of $p$ coefficients

### Assumptions:
* (H1) $rg(\mathbf{X}) = p$
* (H2) $\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2 \mathbf{I}_n)$ 


## Distribution of the estimators

Distribution of $\hat{\boldsymbol{\beta}}$:
$$
\hat{\boldsymbol{\beta}}
\sim
\mathcal{N}\left(
\boldsymbol{\beta};
\sigma^2 (\mathbf{X}^T\mathbf{X})^{-1}
\right)
\quad
\text{and}
\quad
\frac{\hat{\beta}_k - \beta_k}{\sqrt{\hat{\sigma}^2 [(\mathbf{X}^T\mathbf{X})^{-1}]_{kk}}}
\sim
\mathcal{T}_{n-p}.
$$

Distribution of $\hat{\sigma}^2$:
$$
\frac{(n-p) \hat{\sigma}^2}{\sigma^2}
\sim
\chi^2_{n-p}.
$$

## Student t tests

$$
\text{Hypothesis:}
\qquad\qquad
\mathcal{H}_0: \beta_k = 0
\quad
\text{vs}
\quad
\mathcal{H}_1: \beta_k \neq 0
$$

$$
\text{Test Statistic:}
\qquad\qquad\qquad\qquad
T_k 
= \frac{\hat{\beta}_k}{\sqrt{\hat{\sigma}^2_k}}
\underset{\mathcal{H}_0}{\sim}
\mathcal{T}_{n-p}
$$

$$
\text{Reject Region:}
\qquad\qquad\qquad\qquad\qquad\qquad\qquad\quad\\
\mathbb{P}[\text{Reject} | \mathcal{H}_0 \text{ true}] \leq \alpha
\qquad\qquad\qquad\qquad\qquad\\
\qquad\qquad\qquad \iff \mathcal{R}_\alpha = \left\{ t \in \mathbb{R}  ~|~ |t| \geq t_{n-p}(1 - \alpha/2) \right\}
$$

$$
\text{p value:}\qquad\qquad\qquad\qquad\quad
p = \mathbb{P}_{\mathcal{H}_0}[|T_k| > T_k^{obs}]
$$

## Simulated Dataset

$$
y_i = -1 + 3 x_{i1} - x_{i2} + \epsilon_i
$$

```{r test1, echo=TRUE}
set.seed(1289)

## Predictors
n <- 500
x_1 <- runif(n, min = -2, max = 2)
x_2 <- runif(n, min = -2, max = 2)

## Noise
eps <- rnorm(n, mean = 0, sd = 2)

## Model sim
beta_0 <- -1; beta_1 <- 3; beta_2 <- -1
y_sim <- beta_0 + beta_1 * x_1 + beta_2 * x_2 + eps

## Useless predictor
x_junk_1 <- runif(n, min = -2, max = 2)
```

## Simulated Dataset - Fit {.smaller}

$$
y_i = -1 + 3 x_{i1} - x_{i2} + \epsilon_i
$$

```{r test2, echo=TRUE}
fit <- lm(y_sim ~ x_1 + x_2 + x_junk_1)
summary(fit)
```

## Simulated Dataset

$$
y_i = -1 + 3 x_{i1} - x_{i2} + \epsilon_i
$$

```{r test23, echo=TRUE}
## Other Useless Predictors
p_junk <- 100
x_junk <- matrix(runif(n * p_junk, min = -2, max = 2),
                 ncol = p_junk)

## Data frame
data_junk <- data.frame(y_sim = y_sim,
                        x_junk = x_junk)
```

## Simulated Dataset - Fit - Junk {.smaller}

```{r test4, echo=TRUE}
fit <- lm(y_sim ~ -1 + ., data = data_junk)
summary(fit)
```

## Simulated Dataset - Fit - Junk {.smaller}

```{r test5, echo=TRUE}
p_values <- summary(fit)$coefficients[, 4]
hist(p_values, col = "lightblue", breaks = 20)
```

## Simulated Dataset - Fit - Junk {.smaller}

```{r test6, echo=TRUE}
sum(p_values <= 0.05)
summary(fit)$coefficients[p_values <= 0.05, ]
```


$~$

Reject Region: $\mathbb{P}[\text{Reject} | \mathcal{H}_0 \text{ true}] \leq \alpha$

**Just by chance, about 5% of false discoveries.**

<!-- ************************************************************************ -->
# Joint Distribution of the Coefficients - $\sigma^2$ unknown
<!-- ************************************************************************ -->

## Joint distribution of $\hat{\boldsymbol{\beta}}$

When $\sigma^2$ is known:

$$
\hat{\boldsymbol{\beta}}
\sim
\mathcal{N}\left(
\boldsymbol{\beta};
\sigma^2 (\mathbf{X}^T\mathbf{X})^{-1}
\right).
$$

When $\sigma^2$ is **unknown**:
$$
\frac{1}{p\hat{\sigma}^2}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)^T (\mathbf{X}^T\mathbf{X})\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)
\sim
\mathcal{F}^p_{n-p}
$$

with $\mathcal{F}^p_{n-p}$ the **Fisher** distribution.

## Reminder: Fisher Distribution

Let $U_1 \sim \chi^2_{p_1}$ and $U_2 \sim \chi^2_{p_2}$,
$U_1$ and $U_2$ independent.
Then 
$$
F = \frac{U_1/p_1}{U_2/p_2} \sim \mathcal{F}^{p_1}_{p_2}
$$ 

```{r fisher, echo=FALSE, fig.height=4, fig.width=7, fig.align='center'}
x <- 0:600 / 100
ccc <- hcl.colors(6)
par(mar = c(5, 4, 1, 2) + 0.1)
# 1
p1 <- 2
p2 <- 3
plot(x, df(x, p1, p2), type = "l", ylab = "f(x)", ylim = c(0, 1), col = ccc[1], lwd = 2)
# p2
p2 <- 10
lines(x, df(x, p1, p2), col = ccc[5], lwd = 2)
# p1
p1 <- 10
lines(x, df(x, p1, p2), col = ccc[2], lwd = 2)
# p1
p1 <- 20
lines(x, df(x, p1, p2), col = ccc[4], lwd = 2)
# legend
legend("topright",
       c(expression(paste(p[1] == 2, "   ; ", p[2] == 3)),
         expression(paste(p[1] == 2, "   ; ", p[2] == 10)),
         expression(paste(p[1] == 10, "   ; ", p[2] == 10)),
         expression(paste(p[1] == 20, "   ; ", p[2] == 10))),
       col = ccc[c(1, 5, 2, 4)],
       lwd = 2, lty = 1)
```

## Joint distribution of $\hat{\boldsymbol{\beta}}$ - Proof - Hints

Let's show:
$$
\frac{1}{p\hat{\sigma}^2}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)^T (\mathbf{X}^T\mathbf{X})\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)
\sim
\mathcal{F}^p_{n-p}
$$

**Hints:**  
$$
\hat{\boldsymbol{\beta}}
\sim
\mathcal{N}\left(
\boldsymbol{\beta};
\sigma^2 (\mathbf{X}^T\mathbf{X})^{-1}
\right)
\quad
\text{and}
\quad
\frac{(n-p) \hat{\sigma}^2}{\sigma^2}
\sim
\chi^2_{n-p}
$$

$$
\text{If } \mathbf{X} \sim \mathcal{N}(\boldsymbol{\mu}, \mathbf{\Sigma})
\quad
\text{then}
\quad
(\mathbf{X} - \boldsymbol{\mu})^T \mathbf{\Sigma}^{-1} (\mathbf{X} - \boldsymbol{\mu})^T
\sim \chi^2_p
$$ 

$$
\text{If }
U_1 \sim \chi^2_{p_1}
\text{ and }
U_2 \sim \chi^2_{p_2}
\text{ independent, then }
\frac{U_1/p_1}{U_2/p_2} \sim \mathcal{F}^{p_1}_{p_2}
$$ 

## Joint distribution of $\hat{\boldsymbol{\beta}}$ - Proof {.build}

$$
\hat{\boldsymbol{\beta}}
\sim
\mathcal{N}\left(
\boldsymbol{\beta};
\sigma^2 (\mathbf{X}^T\mathbf{X})^{-1}
\right)
\qquad\qquad
\\
\qquad\qquad
\implies
\frac{1}{\sigma^2}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)^T (\mathbf{X}^T\mathbf{X})\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)
\sim
\chi^2_{p}
$$

$$
\frac{(n-p) \hat{\sigma}^2}{\sigma^2}
\sim
\chi^2_{n-p}
\quad
\text{ independent from } \hat{\boldsymbol{\beta}}
$$

Hence:
$$
\begin{aligned}
\frac{
\frac{1}{\sigma^2}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)^T (\mathbf{X}^T\mathbf{X})\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right) / p
}{
\frac{(n-p) \hat{\sigma}^2}{\sigma^2} / (n-p)
}
&=
\frac{1}{p\hat{\sigma}^2}\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)^T (\mathbf{X}^T\mathbf{X})\left(\hat{\boldsymbol{\beta}} - \boldsymbol{\beta}\right)\\
&\sim
\mathcal{F}^p_{n-p}.
\end{aligned}
$$

<!-- ************************************************************************ -->
# Global Fisher F-test
<!-- ************************************************************************ -->

## Global Fisher F-test

Hypothesis:
$$
\mathcal{H}_0: \beta_1 = \cdots = \beta_p = 0
\quad
\text{vs}
\quad
\mathcal{H}_1: \exists~k ~|~ \beta_k \neq 0
$$

Test Statistic:
$$
F
= \frac{1}{p\hat{\sigma}^2}\hat{\boldsymbol{\beta}}^T (\mathbf{X}^T\mathbf{X})\hat{\boldsymbol{\beta}}
\underset{\mathcal{H}_0}{\sim}
\mathcal{F}^p_{n-p}
$$

Reject Region:
$$
\mathbb{P}[\text{Reject} | \mathcal{H}_0 \text{ true}] \leq \alpha
\!\iff\! \mathcal{R}_\alpha = \left\{ f \in \mathbb{R}_+  ~|~ f \geq f^p_{n-p}(1 - \alpha) \right\}
$$

p value:
$$
p = \mathbb{P}_{\mathcal{H}_0}[F > F^{obs}]
$$

## Simulated Dataset - Fit - Junk {.smaller}

```{r test7, echo=TRUE}
fit <- lm(y_sim ~ -1 + ., data = data_junk)
f_obs <- summary(fit)$fstatistic
f_obs
p_value_f <- 1 - pf(f_obs[1], f_obs[2], f_obs[3])
p_value_f
```

We do not reject the null that all the (junk) coefficients are zero (phew).

## Fisher F-Test - Geometry

$$
\begin{aligned}
F
&= \frac{1}{p\hat{\sigma}^2}\hat{\boldsymbol{\beta}}^T (\mathbf{X}^T\mathbf{X})\hat{\boldsymbol{\beta}}
= \frac{(\mathbf{X}\hat{\boldsymbol{\beta}})^T (\mathbf{X}\hat{\boldsymbol{\beta}}) / p}{\|\hat{\boldsymbol{\epsilon}}\|^2 / (n - p)} \\
&= \frac{\|\hat{\mathbf{y}}\|^2 / p}{\|\hat{\boldsymbol{\epsilon}}\|^2 / (n - p)}
= \frac{\|\hat{\mathbf{y}}\|^2 / p}{\|\mathbf{y} -  \hat{\mathbf{y}}\|^2 / (n - p)} \\
\end{aligned}
$$

>* $\mathcal{H}_0: \beta_1 = \cdots = \beta_p = 0$ i.e. the model is useless.

>* If $\|\hat{\boldsymbol{\epsilon}}\|^2$ is **small**, then the error is **small**,  
and $F$ tends to be **large**, i.e. we tend to **reject** $\mathcal{H}_0$:  
the model is usefull.

>* If $\|\hat{\boldsymbol{\epsilon}}\|^2$ is **large**, then the error is **large**,  
and $F$ tends to be **small**, i.e. we tend to **not reject** $\mathcal{H}_0$:  
the model is rather useless.

## Geometrical interpretation

<div class="columns-2">
**Good model** 

```{r proj2, echo=FALSE, fig.height=2.5, fig.width=2.5, fig.align='center'}
par(mar = c(0, 0, 0, 0) + 0.1)
plot.new()
plot.window(c(0, 1.1), c(0, 1.1))
polygon(x = c(0, 0.6, 1, 0.4), y = c(0.2, 0, 0.4, 0.6), col = "gray90")
or <- c(0.3, 0.2)
ybaronev <- c(0.55, 0.45)
yv <- c(0.8, 1.0)
xv <- c(0.5, 0.1 + 0.1/3)
betaxv <- c(0.6, 0.1)
onev <- c(0.6, 0.5)
betaonev <- c(0.5, 0.4)
# ybaronev <- c(0.7, 0.2)
yhatv <- c(0.8, 0.3)
# Y
arrows(x0 = or[1], y0 = or[2], x1 = yv[1], y1 = yv[2],
       length = 0.1, col = "firebrick", lwd = 2)
text(yv[1], yv[2], labels = expression(bold(y)), col = "firebrick", pos = 2)
# yhat
arrows(x0 = or[1], y0 = or[2], x1 = yhatv[1], y1 = yhatv[2],
       length = 0.1, col = "dodgerblue2", lwd = 2)
text(yhatv[1], yhatv[2],
     labels = expression(hat(bold(y))),
     col = "dodgerblue2", pos = 4)
# y - yhat
segments(x0 = yv[1], y0 = yv[2], x1 = yhatv[1], y1 = yhatv[2],
         col = "dodgerblue2", lwd = 1, lty = 2)
# hat epsilon
text((yv[1] + yhatv[1])/2, (yv[2] + yhatv[2])/2,
     labels = expression(hat(bold(epsilon))), col = "dodgerblue2", pos = 4)
```

* $\hat{\mathbf{y}}$ **not too far** from $\mathcal{M}(\mathbf{X})$.

* $\|\hat{\boldsymbol{\epsilon}}\|^2$ **not too large** compared to $\|\hat{\mathbf{y}}\|^2$.

* $F$ tends to be large.

$~$

**Bad model** 

```{r proj1, echo=FALSE, fig.height=2.5, fig.width=2.5, fig.align='center'}
par(mar = c(0, 0, 0, 0) + 0.1)
plot.new()
plot.window(c(0, 1.1), c(0, 1.1))
polygon(x = c(0, 0.6, 1, 0.4), y = c(0.2, 0, 0.4, 0.6), col = "gray90")
or <- c(0.3, 0.2)
ybaronev <- c(0.65, 0.45)
yv <- c(0.4, 1.0)
xv <- c(0.5, 0.1 + 0.1/3)
betaxv <- c(0.6, 0.1)
onev <- c(0.6, 0.5)
betaonev <- c(0.5, 0.4)
# ybaronev <- c(0.7, 0.2)
yhatv <- c(0.4, 0.2)
# Y
arrows(x0 = or[1], y0 = or[2], x1 = yv[1], y1 = yv[2],
       length = 0.1, col = "firebrick", lwd = 2)
text(yv[1], yv[2], labels = expression(bold(y)), col = "firebrick", pos = 2)
# yhat
arrows(x0 = or[1], y0 = or[2], x1 = yhatv[1], y1 = yhatv[2],
       length = 0.1, col = "dodgerblue2", lwd = 2)
text(yhatv[1], yhatv[2],
     labels = expression(hat(bold(y))),
     col = "dodgerblue2", pos = 4)
# y - yhat
segments(x0 = yv[1], y0 = yv[2], x1 = yhatv[1], y1 = yhatv[2],
         col = "dodgerblue2", lwd = 1, lty = 2)
# hat epsilon
text((yv[1] + yhatv[1])/2, (yv[2] + yhatv[2])/2,
     labels = expression(hat(bold(epsilon))), col = "dodgerblue2", pos = 4)
```

* $\hat{\mathbf{y}}$ **almost orth.** to $\mathcal{M}(\mathbf{X})$.

* $\|\hat{\boldsymbol{\epsilon}}\|^2$ **rather large** compared to $\|\hat{\mathbf{y}}\|^2$.

* $F$ tends to be small.

</div>

## Simulated Dataset - True and Junk {.smaller}

```{r test8, echo=TRUE}
## Data frame
data_all <- data.frame(y_sim = y_sim,
                       x_1 = x_1,
                       x_2 = x_2,
                       x_junk = x_junk)
## Fit
fit <- lm(y_sim ~ ., data = data_all)
## multiple t tests
p_values_t <- summary(fit)$coefficients[, 4]
names(p_values_t[p_values_t <= 0.05])

## f test
f_obs <- summary(fit)$fstatistic
p_value_f <- 1 - pf(f_obs[1], f_obs[2], f_obs[3])
p_value_f
```

<!-- ************************************************************************ -->
# Nested Models Fisher F-test
<!-- ************************************************************************ -->

## Nested Models

Can I test ?
$$
\begin{aligned}
\mathcal{H}_0&: \beta_{p_0 + 1} = \cdots = \beta_p = 0 &\text{vs}\\
\mathcal{H}_1&: \exists~k\in \{p_0+1, \dotsc, p\} ~|~ \beta_k \neq 0
\end{aligned}
$$

i.e. decide between the full model:
$$
\mathbf{y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
$$

and the nested model:
$$
\mathbf{y} = \mathbf{X}_0\boldsymbol{\beta}_0 + \boldsymbol{\epsilon}
$$

with $\mathbf{X} = (\mathbf{X}_0 ~ \mathbf{X}_1)$,
$rg(\mathbf{X}) = p$ and
$rg(\mathbf{X}_0) = p_0$.

## Geometrical interpretation

```{r proj3, echo=FALSE, fig.height=5, fig.width=5, fig.align='center'}
par(mar = c(0, 0, 0, 0) + 0.1)
plot.new()
plot.window(c(0, 1.1), c(0, 1.1))
polygon(x = c(0, 0.6, 1, 0.4), y = c(0.2, 0, 0.4, 0.6), col = "gray90")
or <- c(0.3, 0.2)
ybaronev <- c(0.5, 0.4)
yv <- c(0.8, 1.0)
xv <- c(0.5, 0.1 + 0.1/3)
betaxv <- c(0.6, 0.1)
onev <- c(0.6, 0.5)
betaonev <- c(0.5, 0.4)
# ybaronev <- c(0.7, 0.2)
yhatv <- c(0.8, 0.3)
# Y
arrows(x0 = or[1], y0 = or[2], x1 = yv[1], y1 = yv[2],
       length = 0.1, col = "firebrick", lwd = 2)
text(yv[1], yv[2], labels = expression(bold(y)), col = "firebrick", pos = 2)
# yhat
arrows(x0 = or[1], y0 = or[2], x1 = yhatv[1], y1 = yhatv[2],
       length = 0.1, col = "dodgerblue2", lwd = 2)
text(yhatv[1], yhatv[2],
     labels = expression(hat(bold(y))),
     col = "dodgerblue2", pos = 4)
# y - yhat
segments(x0 = yv[1], y0 = yv[2], x1 = yhatv[1], y1 = yhatv[2],
         col = "dodgerblue2", lwd = 1, lty = 2)
# hat epsilon
text((yv[1] + yhatv[1])/2, (yv[2] + yhatv[2])/2,
     labels = expression(hat(bold(epsilon))), col = "dodgerblue2", pos = 4)
# M_0
down <- c(0.25, 0.15)
up <- c(0.6, 0.5)
segments(x0 = down[1], y0 = down[2], x1 = up[1], y1 = up[2],
         col = "darkblue", lwd = 3)
# yhat 0
text(ybaronev[1], ybaronev[2],
     labels = expression(hat(bold(y))[0]),
     col = "darkblue", pos = 2)
# epsilon hat 0
segments(x0 = yv[1], y0 = yv[2], x1 = ybaronev[1], y1 = ybaronev[2],
         col = "darkblue", lwd = 3, lty = 3)
text((yv[1] + ybaronev[1])/2, (yv[2] + ybaronev[2])/2,
     labels = expression(hat(bold(epsilon))[0]), col = "darkblue", pos = 4)
# yhat yhat 0
segments(x0 = ybaronev[1], y0 = ybaronev[2], x1 = yhatv[1], y1 = yhatv[2],
         col = "limegreen", lwd = 3, lty = 3)
```

## Nested Models

**Idea**: Compare

* $\|\hat{\mathbf{y}} -  \hat{\mathbf{y}}_0\|^2$ distance
of $\hat{\mathbf{y}}$ compared to $\hat{\mathbf{y}}_0$ to

* $\|\mathbf{y} - \hat{\mathbf{y}}\|^2$ residual error.

Then:

>* If $\|\hat{\mathbf{y}} - \hat{\mathbf{y}}_0\|^2$
**small** compared to $\|\mathbf{y} -  \hat{\mathbf{y}}\|^2$,
then "$\hat{\mathbf{y}} \approx \hat{\mathbf{y}}_0$" and the null
model is enough.

>* If $\|\hat{\mathbf{y}} - \hat{\mathbf{y}}_0\|^2$
**large** compared to $\|\mathbf{y} - \hat{\mathbf{y}}\|^2$,
then the full model is useful.

## Geometrical interpretation

<div class="columns-2">
**Good model** 

```{r proj4, echo=FALSE, fig.height=2.5, fig.width=2.5, fig.align='center'}
par(mar = c(0, 0, 0, 0) + 0.1)
plot.new()
plot.window(c(0, 1.1), c(0, 1.1))
polygon(x = c(0, 0.6, 1, 0.4), y = c(0.2, 0, 0.4, 0.6), col = "gray90")
or <- c(0.3, 0.2)
ybaronev <- c(0.5, 0.4)
yv <- c(0.8, 1.0)
xv <- c(0.5, 0.1 + 0.1/3)
betaxv <- c(0.6, 0.1)
onev <- c(0.6, 0.5)
betaonev <- c(0.5, 0.4)
# ybaronev <- c(0.7, 0.2)
yhatv <- c(0.8, 0.3)
# Y
arrows(x0 = or[1], y0 = or[2], x1 = yv[1], y1 = yv[2],
       length = 0.1, col = "firebrick", lwd = 2)
text(yv[1], yv[2], labels = expression(bold(y)), col = "firebrick", pos = 2)
# yhat
arrows(x0 = or[1], y0 = or[2], x1 = yhatv[1], y1 = yhatv[2],
       length = 0.1, col = "dodgerblue2", lwd = 2)
text(yhatv[1], yhatv[2],
     labels = expression(hat(bold(y))),
     col = "dodgerblue2", pos = 4)
# y - yhat
segments(x0 = yv[1], y0 = yv[2], x1 = yhatv[1], y1 = yhatv[2],
         col = "dodgerblue2", lwd = 1, lty = 2)
# hat epsilon
# text((yv[1] + yhatv[1])/2, (yv[2] + yhatv[2])/2,
#      labels = expression(hat(bold(epsilon))), col = "dodgerblue2", pos = 4)
# M_0
down <- c(0.25, 0.15)
up <- c(0.6, 0.5)
segments(x0 = down[1], y0 = down[2], x1 = up[1], y1 = up[2],
         col = "darkblue", lwd = 3)
# yhat 0
text(ybaronev[1], ybaronev[2],
     labels = expression(hat(bold(y))[0]),
     col = "darkblue", pos = 2)
# epsilon hat 0
segments(x0 = yv[1], y0 = yv[2], x1 = ybaronev[1], y1 = ybaronev[2],
         col = "darkblue", lwd = 3, lty = 3)
# text((yv[1] + ybaronev[1])/2, (yv[2] + ybaronev[2])/2,
#      labels = expression(hat(bold(epsilon))[0]), col = "darkblue", pos = 4)
# yhat yhat 0
segments(x0 = ybaronev[1], y0 = ybaronev[2], x1 = yhatv[1], y1 = yhatv[2],
         col = "limegreen", lwd = 3, lty = 3)
```

* $\hat{\mathbf{y}}$ **quite different** from $\hat{\mathbf{y}}_0$.

* Full model **does add** information.

$~$

**Bad model** 

```{r proj5, echo=FALSE, fig.height=2.5, fig.width=2.5, fig.align='center'}
par(mar = c(0, 0, 0, 0) + 0.1)
plot.new()
plot.window(c(0, 1.1), c(0, 1.1))
polygon(x = c(0, 0.6, 1, 0.4), y = c(0.2, 0, 0.4, 0.6), col = "gray90")
or <- c(0.3, 0.2)
ybaronev <- c(0.45, 0.35)
yv <- c(0.5, 1.0)
xv <- c(0.5, 0.1 + 0.1/3)
betaxv <- c(0.6, 0.1)
onev <- c(0.6, 0.5)
betaonev <- c(0.5, 0.4)
# ybaronev <- c(0.7, 0.2)
yhatv <- c(0.5, 0.3)
# Y
arrows(x0 = or[1], y0 = or[2], x1 = yv[1], y1 = yv[2],
       length = 0.1, col = "firebrick", lwd = 2)
text(yv[1], yv[2], labels = expression(bold(y)), col = "firebrick", pos = 2)
# yhat
arrows(x0 = or[1], y0 = or[2], x1 = yhatv[1], y1 = yhatv[2],
       length = 0.1, col = "dodgerblue2", lwd = 2)
text(yhatv[1], yhatv[2],
     labels = expression(hat(bold(y))),
     col = "dodgerblue2", pos = 4)
# y - yhat
segments(x0 = yv[1], y0 = yv[2], x1 = yhatv[1], y1 = yhatv[2],
         col = "dodgerblue2", lwd = 1, lty = 2)
# hat epsilon
# text((yv[1] + yhatv[1])/2, (yv[2] + yhatv[2])/2,
#      labels = expression(hat(bold(epsilon))), col = "dodgerblue2", pos = 4)
# M_0
down <- c(0.25, 0.15)
up <- c(0.6, 0.5)
segments(x0 = down[1], y0 = down[2], x1 = up[1], y1 = up[2],
         col = "darkblue", lwd = 3)
# yhat 0
text(ybaronev[1], ybaronev[2],
     labels = expression(hat(bold(y))[0]),
     col = "darkblue", pos = 2)
# epsilon hat 0
segments(x0 = yv[1], y0 = yv[2], x1 = ybaronev[1], y1 = ybaronev[2],
         col = "darkblue", lwd = 3, lty = 3)
# text((yv[1] + ybaronev[1])/2, (yv[2] + ybaronev[2])/2,
#      labels = expression(hat(bold(epsilon))[0]), col = "darkblue", pos = 4)
# yhat yhat 0
segments(x0 = ybaronev[1], y0 = ybaronev[2], x1 = yhatv[1], y1 = yhatv[2],
         col = "limegreen", lwd = 3, lty = 3)
```

* $\hat{\mathbf{y}}$ **similar** to $\hat{\mathbf{y}}_0$.

* Full model **does not add** much information.

</div>

## Nested Models - F test

$$
F = \frac{
\|\hat{\mathbf{y}} -  \hat{\mathbf{y}}_0\|^2 / (p - p_0)
}{
\|\mathbf{y} - \hat{\mathbf{y}}\|^2 / (n - p)
}
$$

>* When $F$ is **large**, full model is **useful**.

>* When $F$ is **small**, null model is enough.

>* Distribution of $F$ ?

## Distribution of $F$ - 1/2 {.build}

$$
F = \frac{
\|\hat{\mathbf{y}} -  \hat{\mathbf{y}}_0\|^2 / (p - p_0)
}{
\|\mathbf{y} - \hat{\mathbf{y}}\|^2 / (n - p)
}
$$

Let $\mathbf{P}$ projection on $\mathcal{M}(\mathbf{X})$ and
$\mathbf{P}_0$ projection on $\mathcal{M}(\mathbf{X}_0)$

$$
\hat{\mathbf{y}} -  \hat{\mathbf{y}}_0
= \mathbf{P}\mathbf{y} - \mathbf{P}_0\mathbf{y}
= \mathbf{P}\mathbf{y} - \mathbf{P}_0\mathbf{P}\mathbf{y}
= (\mathbf{I}_n - \mathbf{P}_0) \mathbf{P}\mathbf{y}
= \mathbf{P}_0^\bot \mathbf{P}\mathbf{y}
$$

$$
\mathbf{y} -  \hat{\mathbf{y}}
= (\mathbf{I}_n - \mathbf{P}) \mathbf{y}
= \mathbf{P}^\bot \mathbf{y}
$$

$\hat{\mathbf{y}} -  \hat{\mathbf{y}}_0$ and $\mathbf{y} -  \hat{\mathbf{y}}$ are **orthogonal**  
i.e. their covariance is zero  
i.e. (Gaussian assumption) they are **independent**.

## Distribution of $F$ - 2/2 {.build}

From Cochran's theorem:
$$
\frac{1}{\sigma^2}\|\mathbf{y} -  \hat{\mathbf{y}}\|^2
= \|\mathbf{P}^\bot\mathbf{y}\|^2
= \|\mathbf{P}^\bot\boldsymbol{\epsilon}\|^2
\sim \chi^2_{n-p}
$$

$$
\frac{1}{\sigma^2}\|\hat{\mathbf{y}} -  \hat{\mathbf{y}}_0 - \mathbf{P}_0^\bot \mathbf{P}\mathbf{X}\boldsymbol{\beta}\|^2
= \|\mathbf{P}_0^\bot \mathbf{P}(\mathbf{y} - \mathbf{X}\boldsymbol{\beta})\|^2
\sim \chi^2_{p-p_0}
$$
($\mathbf{P}_0^\bot \mathbf{P}$ is a projection on a space of dimension $p - p_0$)

If $\mathcal{H}_0$ is true, then $\mathbf{P}_0^\bot \mathbf{P}\mathbf{X}\boldsymbol{\beta} = 0$, hence:

$$
F = \frac{
\|\hat{\mathbf{y}} -  \hat{\mathbf{y}}_0\|^2 / (p - p_0)
}{
\|\mathbf{y} - \hat{\mathbf{y}}\|^2 / (n - p)
}
\underset{\mathcal{H}_0}{\sim}
\mathcal{F}^{p - p_0}_{n - p}.
$$

## Nested F test {.build}

From Phythagoras's theorem:
$$
\begin{aligned}
\|\mathbf{y} -  \hat{\mathbf{y}}_0\|^2
&= \|\mathbf{y} - \mathbf{P}\mathbf{y} + \mathbf{P}\mathbf{y} -  \mathbf{P}_0\mathbf{y}\|^2
= \|\mathbf{P}^\bot\mathbf{y} + \mathbf{P}_0^\bot\mathbf{P}\mathbf{y}\|^2\\
&= \|\mathbf{P}^\bot\mathbf{y}\|^2 + \|\mathbf{P}_0^\bot\mathbf{P}\mathbf{y}\|^2
= \|\mathbf{y} -  \hat{\mathbf{y}}\|^2 + \|\hat{\mathbf{y}} -  \hat{\mathbf{y}}_0\|^2\\
\end{aligned}
$$

i.e.
$$
\|\hat{\mathbf{y}} -  \hat{\mathbf{y}}_0\|^2 
= \|\mathbf{y} -  \hat{\mathbf{y}}_0\|^2 - \|\mathbf{y} -  \hat{\mathbf{y}}\|^2
= RSS_0 - RSS
$$

Hence:
$$
F = \frac{
\|\hat{\mathbf{y}} -  \hat{\mathbf{y}}_0\|^2 / (p - p_0)
}{
\|\mathbf{y} - \hat{\mathbf{y}}\|^2 / (n - p)
}
= \frac{n - p}{p - p_0}\frac{RSS_0 - RSS}{RSS}
$$

## Nested F test

To test:
$$
\begin{aligned}
\mathcal{H}_0&: \beta_{p_0 + 1} = \cdots = \beta_p = 0 &\text{vs}\\
\mathcal{H}_1&: \exists~k\in \{p_0+1, \dotsc, p\} ~|~ \beta_k \neq 0
\end{aligned}
$$

we use the $F$ statistics:
$$
F = \frac{
\|\hat{\mathbf{y}} -  \hat{\mathbf{y}}_0\|^2 / (p - p_0)
}{
\|\mathbf{y} - \hat{\mathbf{y}}\|^2 / (n - p)
}
= \frac{n - p}{p - p_0}\frac{RSS_0 - RSS}{RSS}
\underset{\mathcal{H}_0}{\sim}
\mathcal{F}^{p - p_0}_{n - p}.
$$

## Simulated Dataset - True vs Junk {.smaller}

$$
y_i = -1 + 3 x_{i1} - x_{i2} + \epsilon_i
$$

```{r test9, echo=TRUE}
## Data frame
data_all <- data.frame(y_sim = y_sim,
                       x_1 = x_1,
                       x_2 = x_2,
                       x_junk = x_junk)
## Fit true
fit_small <- lm(y_sim ~ x_1 + x_2, data = data_all)
## Fit true and junk
fit_all <- lm(y_sim ~ x_1 + x_2 + x_junk.1 + x_junk.2 + x_junk.3, data = data_all)
## Comparison
anova(fit_small, fit_all)
```

## Simulated Dataset - True vs Junk {.smaller}

$$
y_i = -1 + 3 x_{i1} - x_{i2} + \epsilon_i
$$

```{r test10, echo=TRUE}
## Fit true
fit_small <- lm(y_sim ~ x_1 + x_2, data = data_all)
## Fit all
fit_all <- lm(y_sim ~ ., data = data_all)
## Comparison
anova(fit_small, fit_all)$`Pr(>F)`[2]
```

We do not reject the null hypothesis that the first two variables are enough
to explain the data.

## Full $F$ test

Assuming that we have an intercept, we often test:
$$
\begin{aligned}
\mathcal{H}_0&: \beta_{2} = \cdots = \beta_p = 0 &\text{vs}\\
\mathcal{H}_1&: \exists~k\in \{2, \dotsc, p\} ~|~ \beta_k \neq 0
\end{aligned}
$$

The associated $F$ statistics is:
$$
F = \frac{
\|\hat{\mathbf{y}} -  \bar{y}\mathbb{1}\|^2 / (p - 1)
}{
\|\mathbf{y} - \hat{\mathbf{y}}\|^2 / (n - p)
}
\underset{\mathcal{H}_0}{\sim}
\mathcal{F}^{p - 1}_{n - p}.
$$

That is the default test returned by `R`.

## Full $F$ test {.smaller}

$$
y_i = -1 + 3 x_{i1} - x_{i2} + \epsilon_i
$$

```{r test11, echo=TRUE}
fit <- lm(y_sim ~ x_1 + x_2)
summary(fit)
```

## One coefficient $F$ test

If $p_0 = p-1$, we test:
$$
\begin{aligned}
\mathcal{H}_0&: \beta_p = 0 &\text{vs} \qquad
\mathcal{H}_1&: \beta_p \neq 0
\end{aligned}
$$

The associated $F$ statistics is:
$$
F = \frac{
\|\hat{\mathbf{y}} -  \hat{\mathbf{y}}_{-p}\|^2 / (p - 1)
}{
\|\mathbf{y} - \hat{\mathbf{y}}\|^2 / (n - p)
}
\underset{\mathcal{H}_0}{\sim}
\mathcal{F}^{1}_{n - p}.
$$

It can be shown that:
$$
F = T^2 \quad \text{with} \quad T = \frac{\hat{\beta}_p}{\hat{\sigma}_p}
$$
so that it is equivalent to the $t$-test.

<!-- ************************************************************************ -->
# Variable Selection
<!-- ************************************************************************ -->

## How to choose the "best" model ? {.build}

**Model**:
$$
y_i = \beta_1 x_{i1} + \dotsb + \beta_p x_{ip} + \epsilon_i
$$

**Problem**:  
Can we choose the "best" subset of non-zero $\beta_k$ ?  

I.e. Can we find the predictors that really have an impact on $\mathbf{y}$ ?

$F$-test on all subsets ? Not practical, multiple testing.

**Idea**:  
Find a "score" to asses the quality of a given fit.

## Variable selection - $R^2$

$$
R^2 = 1 - \frac{RSS}{TSS}
$$

```{r test12, echo=TRUE}
## Function to get statistics for one fit
get_stats_fit <- function(fit) {
  sumfit <- summary(fit)
  res <- data.frame(R2 = sumfit$r.squared)
  return(res)
}
```

$$
y_i = -1 + 3 x_{i1} - x_{i2} + \epsilon_i
$$

## Variable selection - $R^2$ {.smaller}

```{r test13, echo=TRUE}
## Fit true
get_stats_fit(lm(y_sim ~ x_1 + x_2, data = data_all))
## Fit bigger
get_stats_fit(lm(y_sim ~ x_1 + x_2 + x_junk.1, data = data_all))
## Fit wrong
get_stats_fit(lm(y_sim ~ x_1 + x_junk.1, data = data_all))
```

**$R^2$ is not enough.**

## Variable selection - all fits {.smaller}

```{r test132, echo=TRUE}
## Only one "junk" predictor
data_sub <- data.frame(y_sim = y_sim,
                       x_1 = x_1,
                       x_2 = x_2,
                       x_junk = x_junk_1)

## Function to get statistics for one set of predictors
get_stats_pred <- function(preds, y, data) {
  fit <- lm(reformulate(preds, y), data = data)
  res <- get_stats_fit(fit)
  res$preds <- paste(preds, collapse = "+")
  return(res)
}

## Function to get all statistics
get_all_stats <- function(data_sub) {
  all_preds <- colnames(data_sub)[-1]
  y <- colnames(data_sub)[1]
  all_combs <- c(combn(all_preds, 1, get_stats_pred, simplify = FALSE, y = y, data = data_sub),
                 combn(all_preds, 2, get_stats_pred, simplify = FALSE, y = y, data = data_sub),
                 combn(all_preds, 3, get_stats_pred, simplify = FALSE, y = y, data = data_sub))
  res <- do.call(rbind, all_combs)
  return(res)
}
```

## Variable selection - $R^2$

```{r test133, echo=TRUE}
get_all_stats(data_sub)
```

**$R^2$ is not enough.**

## Variable selection - $R_a^2$

$$
R_a^2 = 1 - \frac{RSS / (n-p)}{TSS / (n-1)} 
$$

```{r test14, echo=TRUE}
## Function to get statistics
get_stats_fit <- function(fit) {
  sumfit <- summary(fit)
  res <- data.frame(R2 = sumfit$r.squared,
                    R2a = sumfit$adj.r.squared)
  return(res)
}
```

## Variable selection - $R_a^2$ {.smaller}

```{r test15, echo=TRUE}
get_all_stats(data_sub)
```

**$R_a^2$: adjust for the number of predictors.**

<!-- ************************************************************************ -->
# Penalized Criteria
<!-- ************************************************************************ -->

## Adjusted $R^2$

$$
R_a^2 = 1 - \frac{RSS / (n-p)}{TSS / (n-1)} 
$$

* The larger the better.

* When $p$ is bigger, the fit must be really better for $R_a^2$ to raise.

* Intuitive, but not much theoretical justifications.

## Mallow’s $C_p$

$$
C_p = \frac{1}{n} \left( RSS + 2 p \hat{\sigma}^2 \right)
$$

* Penalize for the number of parameters $p$.

* The **smaller** the better.

* Un-biased estimate of the risk.

* Asymptotic theoretical guaranties.

## Akaike Information Criterion - AIC

$$
AIC = -2\ln L + 2p
$$

* $L$ is the likelihood of the model.

* The **smaller** the better.

* Un-biased estimate of the risk.

* Asymptotic theoretical guaranties.

## Bayesian Information Criterion - BIC

$$
BIC = -2\ln L + p\ln(n)
$$

* $L$ is the likelihood of the model.

* The **smaller** the better.

* Based on an approximation of the marginal likelihood.

* Asymptotic theoretical guaranties.

## Simulated Dataset

```{r test17, echo=TRUE}
## Mallow's Cp
C_p <- function(fit) {
  RSS <- sum(fit$residuals^2)
  n <- nobs(fit); p <- n - df.residual(fit)
  sigma2_hat <- RSS / (n-p)
  return((RSS + 2 * p * sigma2_hat) / n)
}

## Function to get statistics
get_stats_fit <- function(fit) {
  sumfit <- summary(fit)
  res <- data.frame(Cp = C_p(fit),
                    AIC = AIC(fit),
                    BIC = BIC(fit))
  return(res)
}
```

## Variable selection {.smaller}

```{r test18, echo=TRUE}
## Apply stats to all possible combinations
all_stats <- get_all_stats(data_sub)
all_stats

## Select model
apply(all_stats[, -ncol(all_stats)], 2,
      function(x) all_stats[which.min(x), ncol(all_stats)])
```

## Variable selection - Advertising data {.smaller}

```{r test19, message = FALSE}
## Advertising data
library(here)
ad <- read.csv(here("data", "Advertising.csv"), row.names = "X")

## Apply stats to all possible combinations
all_stats <- get_all_stats(ad)
all_stats

## Select model
apply(all_stats[, -ncol(all_stats)], 2,
      function(x) all_stats[which.min(x), ncol(all_stats)])
```

<!-- ************************************************************************ -->
# Forward and Backward Search
<!-- ************************************************************************ -->

## Combinatorial problem

$$
p \text{ predictors } \to 2^p \text{ possible models.}
$$

>* Cannot test all possible models.

>* Solutions: LASSO and other penalized criteria (see M2)

>* "Manual" solution: forward or backward search

## Forward search

>* Start with the null model (no predictor)

>* Try to **add one** predictor ($p$ models to fit)

>* Choose the best fitting among the $p$ models.

>* Repeat until no more predictors to add.

>* Choose best fit with $C_p$, AIC or BIC.

## Backward search

>* Start with the full model (all predictors)

>* Try to **remove one** predictor ($p$ models to fit)

>* Choose the best fitting among the $p$ models.

>* Repeat until no more predictors to remove.

>* Choose best fit with $C_p$, AIC or BIC.